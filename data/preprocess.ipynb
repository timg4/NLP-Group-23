{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab6fb8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae37d2f",
   "metadata": {},
   "source": [
    "This notebook is for experimentation and exploration only. The normal preprocessing pipeline will be run by the corresponding .py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89b3122e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\timgr\\Desktop\\NLP\\NLP-Group-23\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\timgr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\timgr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 4.29MB/s]                    \n",
      "2025-11-01 19:43:09 INFO: Downloaded file to C:\\Users\\timgr\\stanza_resources\\resources.json\n",
      "2025-11-01 19:43:09 INFO: Downloading these customized packages for language: de (German)...\n",
      "==================================\n",
      "| Processor       | Package      |\n",
      "----------------------------------\n",
      "| tokenize        | gsd          |\n",
      "| mwt             | gsd          |\n",
      "| pos             | gsd_charlm   |\n",
      "| lemma           | gsd_nocharlm |\n",
      "| pretrain        | conll17      |\n",
      "| backward_charlm | newswiki     |\n",
      "| forward_charlm  | newswiki     |\n",
      "==================================\n",
      "\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-de/resolve/v1.9.0/models/tokenize/gsd.pt: 100%|██████████| 655k/655k [00:00<00:00, 9.42MB/s]\n",
      "2025-11-01 19:43:11 INFO: Downloaded file to C:\\Users\\timgr\\stanza_resources\\de\\tokenize\\gsd.pt\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-de/resolve/v1.9.0/models/mwt/gsd.pt: 100%|██████████| 547k/547k [00:00<00:00, 9.19MB/s]\n",
      "2025-11-01 19:43:11 INFO: Downloaded file to C:\\Users\\timgr\\stanza_resources\\de\\mwt\\gsd.pt\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-de/resolve/v1.9.0/models/pos/gsd_charlm.pt: 100%|██████████| 38.1M/38.1M [00:00<00:00, 40.8MB/s]\n",
      "2025-11-01 19:43:13 INFO: Downloaded file to C:\\Users\\timgr\\stanza_resources\\de\\pos\\gsd_charlm.pt\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-de/resolve/v1.9.0/models/lemma/gsd_nocharlm.pt: 100%|██████████| 5.69M/5.69M [00:00<00:00, 28.0MB/s]\n",
      "2025-11-01 19:43:15 INFO: Downloaded file to C:\\Users\\timgr\\stanza_resources\\de\\lemma\\gsd_nocharlm.pt\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-de/resolve/v1.9.0/models/pretrain/conll17.pt: 100%|██████████| 107M/107M [00:02<00:00, 45.9MB/s] \n",
      "2025-11-01 19:43:19 INFO: Downloaded file to C:\\Users\\timgr\\stanza_resources\\de\\pretrain\\conll17.pt\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-de/resolve/v1.9.0/models/backward_charlm/newswiki.pt: 100%|██████████| 20.0M/20.0M [00:00<00:00, 39.6MB/s]\n",
      "2025-11-01 19:43:20 INFO: Downloaded file to C:\\Users\\timgr\\stanza_resources\\de\\backward_charlm\\newswiki.pt\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-de/resolve/v1.9.0/models/forward_charlm/newswiki.pt: 100%|██████████| 20.0M/20.0M [00:00<00:00, 37.6MB/s]\n",
      "2025-11-01 19:43:22 INFO: Downloaded file to C:\\Users\\timgr\\stanza_resources\\de\\forward_charlm\\newswiki.pt\n",
      "2025-11-01 19:43:22 INFO: Finished downloading models and saved to C:\\Users\\timgr\\stanza_resources\n"
     ]
    }
   ],
   "source": [
    "import nltk, stanza\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "stanza.download(\"de\", package=\"gsd\", processors=\"tokenize,mwt,pos,lemma\")  # German model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7caad897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"fincorpus-de-10k.py\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36467135",
   "metadata": {},
   "source": [
    "For the sake of exploration and trying out different stuff, we will use a small subset of the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1aac41c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = dataset.shuffle(seed=42).select(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10cab59a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>char_len</th>\n",
       "      <th>num_lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>txt/Final_terms/DE000DB9U1Y1.pdf.txt</td>\n",
       "      <td>Endgültige Bedingungen Nr. 11 vom 16. Juni 202...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64451.100000</td>\n",
       "      <td>1833.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32495.523321</td>\n",
       "      <td>977.078662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10266.000000</td>\n",
       "      <td>476.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45457.250000</td>\n",
       "      <td>1101.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55709.000000</td>\n",
       "      <td>1573.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86111.500000</td>\n",
       "      <td>2486.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>122304.000000</td>\n",
       "      <td>3480.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    filename  \\\n",
       "count                                     10   \n",
       "unique                                    10   \n",
       "top     txt/Final_terms/DE000DB9U1Y1.pdf.txt   \n",
       "freq                                       1   \n",
       "mean                                     NaN   \n",
       "std                                      NaN   \n",
       "min                                      NaN   \n",
       "25%                                      NaN   \n",
       "50%                                      NaN   \n",
       "75%                                      NaN   \n",
       "max                                      NaN   \n",
       "\n",
       "                                                     text       char_len  \\\n",
       "count                                                  10      10.000000   \n",
       "unique                                                 10            NaN   \n",
       "top     Endgültige Bedingungen Nr. 11 vom 16. Juni 202...            NaN   \n",
       "freq                                                    1            NaN   \n",
       "mean                                                  NaN   64451.100000   \n",
       "std                                                   NaN   32495.523321   \n",
       "min                                                   NaN   10266.000000   \n",
       "25%                                                   NaN   45457.250000   \n",
       "50%                                                   NaN   55709.000000   \n",
       "75%                                                   NaN   86111.500000   \n",
       "max                                                   NaN  122304.000000   \n",
       "\n",
       "          num_lines  \n",
       "count     10.000000  \n",
       "unique          NaN  \n",
       "top             NaN  \n",
       "freq            NaN  \n",
       "mean    1833.400000  \n",
       "std      977.078662  \n",
       "min      476.000000  \n",
       "25%     1101.250000  \n",
       "50%     1573.500000  \n",
       "75%     2486.250000  \n",
       "max     3480.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(subset)\n",
    "df[\"char_len\"] = df[\"text\"].str.len()\n",
    "df[\"num_lines\"] = df[\"text\"].str.count(r\"\\n\") + 1\n",
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66015ce",
   "metadata": {},
   "source": [
    "we can see here that some of our files are already very large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1254a8ad",
   "metadata": {},
   "source": [
    "# domain specific normalization\n",
    "We want to keep Abbreviations that cause sentence-break carnage. For that we will replace dots with ⟂ in the abbreviations that we know and revert that after tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5650cde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ABBREV = [\n",
    "    r\"Abs\\.\", r\"Nr\\.\", r\"Art\\.\", r\"Kap\\.\", r\"Anm\\.\", r\"z\\. ?B\\.\", r\"u\\. ?a\\.\", r\"vgl\\.\", r\"ca\\.\", r\"bzw\\.\",\n",
    "    r\"Dr\\.\", r\"Dipl\\.\", r\"Prof\\.\", r\"Hr\\.\", r\"Fr\\.\", r\"i\\. ?V\\.\", r\"i\\. ?S\\.\", r\"i\\. ?d\\. ?R\\.\",\n",
    "]\n",
    "ABBREV_RX = re.compile(r\"(\" + \"|\".join(ABBREV) + r\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8943cabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def protect_abbrev(text: str) -> str:\n",
    "    #replace the '.' in known abbreviations with '⟂'\n",
    "    def repl(m):\n",
    "        return m.group(0).replace(\".\", \"⟂\")\n",
    "    return ABBREV_RX.sub(repl, text)\n",
    "\n",
    "def unprotect(text: str) -> str:\n",
    "    return text.replace(\"⟂\", \".\")\n",
    "\n",
    "#to keep § while still normalizing spaces\n",
    "def normalize_paragraph_sign(text: str) -> str:\n",
    "    return re.sub(r\"§\\s+\", \"§ \", text)\n",
    "\n",
    "def light_clean(text: str) -> str:\n",
    "    t = text.strip()\n",
    "    t = normalize_paragraph_sign(t)\n",
    "    t = re.sub(r\"[ \\t]+\", \" \", t)          \n",
    "    t = re.sub(r\"\\n{3,}\", \"\\n\\n\", t)        \n",
    "    t = protect_abbrev(t)\n",
    "    return t\n",
    "\n",
    "df[\"text_norm\"] = df[\"text\"].map(light_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f032b2a",
   "metadata": {},
   "source": [
    "# Stanza pipeline and sentence segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa5e962e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-01 19:43:37 WARNING: Language de package default expects mwt, which has been added\n",
      "2025-11-01 19:43:37 INFO: Loading these models for language: de (German):\n",
      "============================\n",
      "| Processor | Package      |\n",
      "----------------------------\n",
      "| tokenize  | gsd          |\n",
      "| mwt       | gsd          |\n",
      "| pos       | gsd_charlm   |\n",
      "| lemma     | gsd_nocharlm |\n",
      "============================\n",
      "\n",
      "2025-11-01 19:43:37 INFO: Using device: cpu\n",
      "2025-11-01 19:43:37 INFO: Loading: tokenize\n",
      "c:\\Users\\timgr\\Desktop\\NLP\\NLP-Group-23\\.venv\\Lib\\site-packages\\stanza\\models\\tokenization\\trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2025-11-01 19:43:38 INFO: Loading: mwt\n",
      "c:\\Users\\timgr\\Desktop\\NLP\\NLP-Group-23\\.venv\\Lib\\site-packages\\stanza\\models\\mwt\\trainer.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2025-11-01 19:43:38 INFO: Loading: pos\n",
      "c:\\Users\\timgr\\Desktop\\NLP\\NLP-Group-23\\.venv\\Lib\\site-packages\\stanza\\models\\pos\\trainer.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "c:\\Users\\timgr\\Desktop\\NLP\\NLP-Group-23\\.venv\\Lib\\site-packages\\stanza\\models\\common\\pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "c:\\Users\\timgr\\Desktop\\NLP\\NLP-Group-23\\.venv\\Lib\\site-packages\\stanza\\models\\common\\char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "2025-11-01 19:43:39 INFO: Loading: lemma\n",
      "c:\\Users\\timgr\\Desktop\\NLP\\NLP-Group-23\\.venv\\Lib\\site-packages\\stanza\\models\\lemma\\trainer.py:239: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2025-11-01 19:43:39 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(\n",
    "    \"de\", processors=\"tokenize,pos,lemma\",\n",
    "    tokenize_no_ssplit=False,  \n",
    "    use_gpu=False, download_method=stanza.DownloadMethod.REUSE_RESOURCES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ae3e690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 8),\n",
       " (1, 35),\n",
       " (2, 17),\n",
       " (3, 40),\n",
       " (4, 19),\n",
       " (5, 13),\n",
       " (6, 5),\n",
       " (7, 37),\n",
       " (8, 31),\n",
       " (9, 15)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def annotate(text: str):\n",
    "    #if stanza blows up on pathological monstrosities, slice or skip\n",
    "    return nlp(text)\n",
    "\n",
    "\n",
    "doc = annotate(df[\"text_norm\"].iloc[0])\n",
    "[(i, len(s.words)) for i, s in enumerate(doc.sentences)][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97a9a47",
   "metadata": {},
   "source": [
    "## lets look at some and see if our sentence segmentation works properly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55ded2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting docs: [6, 9]\n",
      "\n",
      "================= DOC 6 =================\n",
      "Datum der Endgültigen Bedingungen und des ersten öffentlichen Angebots: 17.01.2023 \n",
      " \n",
      " \n",
      "ENDGÜLTIGE BEDINGUNGEN \n",
      "Landesbank Baden-Württemberg \n",
      "(LEI: B81CK4ESI35472RHJ606) \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "24.000.000 EUR \n",
      "LBBW 3,41 % Festzins-Anleihe \n",
      "festverzinsliche Schuldverschreibungen \n",
      "(die \"Schuldverschreibungen\") \n",
      " \n",
      "ISIN-Code: DE000LB38W46 \n",
      " \n",
      " \n",
      " \n",
      "emittiert unter dem \n",
      " \n",
      "Angebotsprogramm zur Emission von Schuldverschreibungen und Pfandbriefen \n",
      " \n",
      "Die Gültigkeit des Basisprospekts der Landesbank Baden-Württemberg (die  ...\n",
      "\n",
      "Num sentences: 312\n",
      "\n",
      "--- Sentence 1 ---\n",
      "Datum der Endgültigen Bedingungen und des ersten öffentlichen Angebots : 17.01.2023\n",
      "\n",
      "--- Sentence 2 ---\n",
      "ENDGÜLTIGE BEDINGUNGEN Landesbank Baden - Württemberg ( LEI : B81CK4ESI35472RHJ606 )\n",
      "\n",
      "--- Sentence 3 ---\n",
      "24.000.000 EUR LBBW 3,41 % Festzins - Anleihe festverzinsliche Schuldverschreibungen ( die \" Schuldverschreibungen \" )\n",
      "\n",
      "================= DOC 9 =================\n",
      "Endgültige Bedingungen Nr⟂ 397 vom 01.04.2022\n",
      "zum Basisprospekt B vom 27. April 2021\n",
      "geändert durch den Nachtrag Nr⟂ 1 vom 12. Mai 2021,\n",
      "Nachtrag Nr⟂ 2 vom 13. Juli 2021,\n",
      "Nachtrag Nr⟂ 3 vom 15. Juli 2021,\n",
      "Nachtrag Nr⟂ 4 vom 08. September 2021,\n",
      "Nachtrag Nr⟂ 5 vom 03. Januar 2022 und\n",
      "Nachtrag Nr⟂ 6 vom 29. März 2022\n",
      "Endgültige Bedingungen\n",
      "für\n",
      "Festverzinsliche Schuldverschreibungen\n",
      "Die Schuldverschreibungen werden unter folgendem Namen vermarktet:\n",
      "\"Carrara Festzinsanleihe 04w/22-04/32 mit Kündigung ...\n",
      "\n",
      "Num sentences: 393\n",
      "\n",
      "--- Sentence 1 ---\n",
      "Endgültige Bedingungen Nr⟂ 397 von dem 01.04.2022 zu dem Basisprospekt B von dem 27 .\n",
      "\n",
      "--- Sentence 2 ---\n",
      "April 2021 geändert durch den Nachtrag Nr⟂ 1 von dem 12 .\n",
      "\n",
      "--- Sentence 3 ---\n",
      "Mai 2021 , Nachtrag Nr⟂ 2 von dem 13. Juli 2021 , Nachtrag Nr⟂ 3 von dem 15 .\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "sample_indices = random.sample(range(len(df)), 2)\n",
    "print(\"Inspecting docs:\", sample_indices)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    text = df.loc[idx, \"text_norm\"]\n",
    "    print(f\"\\n================= DOC {idx} =================\")\n",
    "    print(text[:500], \"...\\n\")  \n",
    "\n",
    "    doc = nlp(text)\n",
    "    print(f\"Num sentences: {len(doc.sentences)}\")\n",
    "    for i, s in enumerate(doc.sentences[:3]):  \n",
    "        print(f\"\\n--- Sentence {i+1} ---\")\n",
    "        print(\" \".join([w.text for w in s.words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f391283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence 1 tokens:\n",
      "TEXT            LEMMA           POS       \n",
      "----------------------------------------\n",
      "Datum           Datum           NOUN      \n",
      "der             der             DET       \n",
      "Endgültigen     endgültig       ADJ       \n",
      "Bedingungen     Bedingung       NOUN      \n",
      "und             und             CCONJ     \n",
      "des             der             DET       \n",
      "ersten          erst            ADJ       \n",
      "öffentlichen    öffentlich      ADJ       \n",
      "Angebots        Angebot         NOUN      \n",
      ":               :               PUNCT     \n",
      "17.01.2023      17.01.2023      NUM       \n",
      "\n",
      "Sentence 2 tokens:\n",
      "TEXT            LEMMA           POS       \n",
      "----------------------------------------\n",
      "ENDGÜLTIGE      ENDGÜLTIGE      PROPN     \n",
      "BEDINGUNGEN     BEDINGUNGEN     PROPN     \n",
      "Landesbank      Landesbank      PROPN     \n",
      "Baden           Baden           PROPN     \n",
      "-               -               PUNCT     \n",
      "Württemberg     Württemberg     PROPN     \n",
      "(               (               PUNCT     \n",
      "LEI             LEI             PROPN     \n",
      ":               :               PUNCT     \n",
      "B81CK4ESI35472RHJ606 B81CK4ESI35472RHJ606 PROPN     \n",
      ")               )               PUNCT     \n"
     ]
    }
   ],
   "source": [
    "i = sample_indices[0]\n",
    "doc = nlp(df.loc[i, \"text_norm\"])\n",
    "\n",
    "for s_i, sent in enumerate(doc.sentences[:2]):\n",
    "    print(f\"\\nSentence {s_i+1} tokens:\")\n",
    "    print(f\"{'TEXT':15} {'LEMMA':15} {'POS':10}\")\n",
    "    print(\"-\"*40)\n",
    "    for w in sent.words:\n",
    "        print(f\"{w.text:15} {w.lemma:15} {w.upos:10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab1336c",
   "metadata": {},
   "source": [
    "the processed seemed to have worked fine so we can now export it into CoNLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "446cb84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [08:03<00:00, 48.38s/it]\n"
     ]
    }
   ],
   "source": [
    "from stanza.utils.conll import CoNLL\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "EXPORT_N = len(df)  # or 1000 if you have time\n",
    "\n",
    "out_dir = Path(\"data/processed\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for i in tqdm(range(EXPORT_N)):\n",
    "    text = df.loc[i, \"text_norm\"]\n",
    "    doc = nlp(text)\n",
    "    out_path = out_dir / f\"doc_{i:05d}.conllu\"\n",
    "    CoNLL.write_doc2conll(doc, str(out_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8340da23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\processed\\doc_00000.conllu\n",
      "# text = Endgültige Bedingungen Nr⟂ 11 vom 16.\n",
      "# sent_id = 0\n",
      "1\tEndgültige\tendgültig\tADJ\tADJA\tCase=Nom|Degree=Pos|Gender=Fem|Number=Plur\t0\t_\t_\tstart_char=0|end_char=10\n",
      "2\tBedingungen\tBedingung\tNOUN\tNN\tCase=Nom|Gender=Fem|Number=Plur\t1\t_\t_\tstart_char=11|end_char=22\n",
      "3\tNr⟂\tNr⟂\tPROPN\tNN\tCase=Nom|Gender=Fem|Number=Sing\t2\t_\t_\tstart_char=23|end_char=26\n",
      "4\t11\t11\tPROPN\tCARD\tNumType=Card\t3\t_\t_\tstart_char=27|end_char=29\n",
      "5-6\tvom\t_\t_\t_\t_\t_\t_\t_\tstart_char=30|end_char=33\n",
      "5\tvon\tvon\tADP\tAPPR\t_\t4\t_\t_\t_\n",
      "6\tdem\tder\tDET\tART\tCase=Dat|Definite=Def|Gender=Neut|Number=Sing|PronType=Art\t5\t_\t_\t_\n",
      "7\t16\t16\tNUM\tCARD\tNumType=Card\t6\t_\t_\tstart_char=34|end_char=36|SpaceAfter=No\n",
      "8\t.\t.\tPUNCT\t$.\t_\t7\t_\t_\tstart_char=36|end_char=37\n",
      "\n",
      "# text = Juni 2020 DEUTSCHE BANK AG Emission von bis zu 5.000.000 Marktzinsanleihen (entspricht Produkt-Nr⟂ 41 in der Wertpapierbeschreibung für Schuldverschreibungen) zu je EUR 100,00 mit einem Gesamtnennbetrag von bis zu EUR 500.000.000\n",
      "# sent_id = 1\n",
      "1\tJuni\tJuni\tPROPN\tNN\tCase=Nom|Gender=Masc|Number=Sing\t0\t_\t_\tstart_char=38|end_char=42\n",
      "2\t2020\t2020\tNUM\tCARD\tNumType=Card\t1\t_\t_\tstart_char=43|end_char=47|SpacesAfter=\\s\\n\n",
      "3\tDEUTSCHE\tDEUTSCHE\tPROPN\tNN\tCase=Acc|Gender=Fem|Number=Sing\t2\t_\t_\tstart_char=49|end_char=57\n",
      "4\tBANK\tBANK\tPROPN\tNE\tCase=Nom|Gender=Fem|Number=Sing\t3\t_\t_\tstart_char=58|end_char=62\n",
      "5\tAG\tAG\tPROPN\tNN\tCase=Acc|Gender=Fem|Number=Sing\t4\t_\t_\tstart_char=63|end_char=65|SpacesAfter=\\s\\n\n",
      "6\tEmission\tEmission\tPROPN\tNN\tCase=Acc|Gender=Fem|Number=Sing\t5\t_\t_\tstart_char=67|end_char=75\n",
      "7\tvon\tvon\tADP\tAPPR\t_\t6\t_\t_\tstart_char=76|end_char=79\n",
      "8\tbis\tbis\tADP\tADV\t_\t7\t_\t_\tstart_char=80|end_char=83\n",
      "9\tzu\tzu\tADP\tADV\t_\t8\t_\t_\tstart_char=84|end_char=86\n",
      "10\t5.000.000\t5.000.000\tNUM\tCARD\tNumType=Card\t9\t_\t_\tstart_char=87|end_char=96\n",
      "11\tMarktzinsanleihen\tMarktzinsanleihen\tNOUN\tNN\tCase=Nom|Gender=Fem|Number=Plur\t10\t_\t_\tstart_char=97|end_char=114\n",
      "12\t(\t(\tPUNCT\t$(\t_\t11\t_\t_\tstart_char=115|end_char=116|SpaceAfter=No\n",
      "13\tentspricht\tentsprechen\tVERB\tVVFIN\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\t12\t_\t_\tstart_char=116|end_char=126\n",
      "14\tProdukt\tProdukt\tNOUN\tNN\tCase=Nom|Gender=Neut|Number=Sing\t13\t_\t_\tstart_char=127|end_char=134|SpaceAfter=No\n",
      "15\t-\t-\tPUNCT\t$(\t_\t14\t_\t_\tstart_char=134|end_char=135|SpaceAfter=No\n",
      "16\tNr⟂\tNr⟂\tPROPN\tNN\tCase=Nom|Gender=Neut|Number=Sing\t15\t_\t_\tstart_char=135|end_char=138\n"
     ]
    }
   ],
   "source": [
    "sample_file = sorted(out_dir.glob(\"*.conllu\"))[0]\n",
    "print(sample_file)\n",
    "print(\"\\n\".join(sample_file.read_text(encoding=\"utf-8\").splitlines()[:30]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
