{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13d67e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter, defaultdict\n",
    "from math import log\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "673975be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_conllu(path):\n",
    "    sentences = []\n",
    "    tokens, labels = [], []\n",
    "    \n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            \n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    sentences.append({\"tokens\": tokens, \"labels\": labels})\n",
    "                    tokens, labels = [], []\n",
    "                continue\n",
    "            \n",
    "            #skip the comments where we say which sentence tec\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            \n",
    "            cols = line.split(\"\\t\")\n",
    "\n",
    "            if \"-\" in cols[0] or \".\" in cols[0]:\n",
    "                continue\n",
    "            \n",
    "            token = cols[1]\n",
    "            ner_label = cols[-1] #NER label in the last column\n",
    "            \n",
    "            tokens.append(token)\n",
    "            labels.append(ner_label)\n",
    "    \n",
    "    if tokens:\n",
    "        sentences.append({\"tokens\": tokens, \"labels\": labels})\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "conllu_path = \"C:\\\\Users\\\\timgr\\\\Desktop\\\\NLP\\\\NLP-Group-23\\\\data\\\\manual_annotation\\\\sample_sentences_labeled.conllu\"  # <-- adjust path\n",
    "sentences = load_conllu(conllu_path)\n",
    "len(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b98497",
   "metadata": {},
   "source": [
    "Lets have a look at how are labels are distributed for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cae60e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 150\n",
      "Label distribution:\n",
      "O          3120\n",
      "B-LEG        51\n",
      "I-LEG        82\n",
      "B-ORG        50\n",
      "I-ORG       119\n",
      "B-MON        68\n",
      "I-MON        11\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of sentences:\", len(sentences))\n",
    "\n",
    "all_labels = [lab for s in sentences for lab in s[\"labels\"]]\n",
    "label_counts = Counter(all_labels)\n",
    "print(\"Label distribution:\")\n",
    "for lab, c in label_counts.items():\n",
    "    print(f\"{lab:8s} {c:6d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdf0fc3",
   "metadata": {},
   "source": [
    "# Train Dev split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f63fe0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 30)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set, dev_set = train_test_split(\n",
    "    sentences, test_size=0.2, random_state=23\n",
    ")\n",
    "\n",
    "len(train_set), len(dev_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "814fefaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2810, 691)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flatten_tokens(sentences):\n",
    "    return [\n",
    "        (tok, lab)\n",
    "        for s in sentences\n",
    "        for tok, lab in zip(s[\"tokens\"], s[\"labels\"])\n",
    "    ]\n",
    "\n",
    "train_tokens = flatten_tokens(train_set)\n",
    "dev_tokens   = flatten_tokens(dev_set)\n",
    "\n",
    "len(train_tokens), len(dev_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d6e163",
   "metadata": {},
   "source": [
    "# Lets build a rule based NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1821ab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class SimpleRuleNER:\n",
    "    def __init__(self):\n",
    "        # patterns for monetary stuff\n",
    "        # e.g. 3,00  21.396  2,50%  0%  2,0\n",
    "        self.mon_number_re = re.compile(\n",
    "            r\"^\\d{1,3}(\\.\\d{3})*(,\\d+)?$\"    \n",
    "            r\"|^\\d+(,\\d+)?%$\"                  \n",
    "        )\n",
    "        self.currency_tokens = {\"€\", \"eur\", \"eur.\", \"euro\", \"euro.\"}\n",
    "\n",
    "        # legal patterns\n",
    "        self.leg_starters = {\"§\", \"Artikel\", \"Artikel.\", \"Art.\", \"Art\"}\n",
    "        self.leg_follow = {\n",
    "            \"Absatz\", \"Abs.\", \"Satz\", \"Nr.\", \"Nr\", \"Nummer\",\n",
    "            \"(\", \")\", \"-\", \"WpHG\"\n",
    "        }\n",
    "\n",
    "        # organization patterns, banks etc\n",
    "        self.org_keywords = {\n",
    "            \"bank\", \"bundesbank\", \"sparkasse\", \"genossenschaftsbank\"\n",
    "        }\n",
    "        self.org_suffixes = {\n",
    "            \"ag\", \"gmbh\", \"kg\", \"kgaa\", \"se\", \"plc\", \"ltd\", \"llc\", \"inc.\", \"sarl\"\n",
    "        }\n",
    "\n",
    "    # ------- helpers --------\n",
    "\n",
    "    def _is_numeric_like(self, tok: str) -> bool:\n",
    "        return any(ch.isdigit() for ch in tok)\n",
    "\n",
    "    def predict_sentence(self, tokens):\n",
    "        n = len(tokens)\n",
    "        labels = [\"O\"] * n\n",
    "        lower = [t.lower() for t in tokens]\n",
    "\n",
    "        # rules for leagal stuff\n",
    "        i = 0\n",
    "        while i < n:\n",
    "            t = tokens[i]\n",
    "            tl = lower[i]\n",
    "\n",
    "            # Pattern could be § 123 \n",
    "            if t == \"§\":\n",
    "                labels[i] = \"B-LEG\"\n",
    "                j = i + 1\n",
    "                # then we take a short window of tokens after \n",
    "                while j < n and tokens[j] not in {\".\", \";\"} and j < i + 8:\n",
    "                    labels[j] = \"I-LEG\"\n",
    "                    j += 1\n",
    "                i = j\n",
    "                continue\n",
    "\n",
    "            # Patrern is Artikel ...\n",
    "            if t in {\"Artikel\", \"Artikel.\", \"Art.\", \"Art\"}:\n",
    "                labels[i] = \"B-LEG\"\n",
    "                j = i + 1\n",
    "                #continues with numbers or legal words\n",
    "                while j < n:\n",
    "                    tj = tokens[j]\n",
    "                    tlj = lower[j]\n",
    "                    if self._is_numeric_like(tj) or t in self.leg_follow or tlj in {lf.lower() for lf in self.leg_follow}:\n",
    "                        labels[j] = \"I-LEG\"\n",
    "                        j += 1\n",
    "                    else:\n",
    "                        break\n",
    "                i = j\n",
    "                continue\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        # rulkes for monetary stuff\n",
    "        for i, t in enumerate(tokens):\n",
    "            if labels[i] != \"O\":\n",
    "                #don't overwrite LEG\n",
    "                continue\n",
    "            tl = lower[i]\n",
    "            prev_tok = lower[i-1] if i > 0 else \"\"\n",
    "            next_tok = lower[i+1] if i + 1 < n else \"\"\n",
    "\n",
    "            #Pattern could b numeric with thousand/decimal separators or \"%\"\n",
    "            if self.mon_number_re.match(t) or t.endswith(\"%\"):\n",
    "                labels[i] = \"B-MON\"\n",
    "                # attach directly adjacent % or currency tokens\n",
    "                j = i + 1\n",
    "                while j < n:\n",
    "                    tj = tokens[j]\n",
    "                    tlj = lower[j]\n",
    "                    if (\n",
    "                        tj.endswith(\"%\")\n",
    "                        or tlj in self.currency_tokens\n",
    "                        or self.mon_number_re.match(tj)\n",
    "                    ):\n",
    "                        if labels[j] == \"O\":\n",
    "                            labels[j] = \"I-MON\"\n",
    "                        j += 1\n",
    "                    else:\n",
    "                        break\n",
    "                continue\n",
    "\n",
    "            # Pattern could be currency word with numeric before it\n",
    "            if tl in self.currency_tokens and i > 0 and self._is_numeric_like(tokens[i-1]):\n",
    "                if labels[i-1] == \"O\":\n",
    "                    labels[i-1] = \"B-MON\"\n",
    "                labels[i] = \"I-MON\"\n",
    "\n",
    "        # rules for organizational stuff\n",
    "        for i, t in enumerate(tokens):\n",
    "            if labels[i] != \"O\":\n",
    "                continue\n",
    "            tl = lower[i]\n",
    "\n",
    "            # Pattern could be lexicon hits like BANK\n",
    "            if tl in self.org_keywords:\n",
    "                labels[i] = \"B-ORG\"\n",
    "                j = i + 1\n",
    "                while j < n:\n",
    "                    tj = tokens[j]\n",
    "                    tlj = lower[j]\n",
    "                    #extend through typical name pieces\n",
    "                    if (\n",
    "                        tj[0].isupper()\n",
    "                        or tlj in self.org_keywords\n",
    "                        or tlj in self.org_suffixes\n",
    "                        or tj in {\"-\", \"–\", \"&\", \"’s\", \"'s\", \",\", \".\"}\n",
    "                    ):\n",
    "                        if labels[j] == \"O\":\n",
    "                            labels[j] = \"I-ORG\"\n",
    "                        j += 1\n",
    "                    else:\n",
    "                        break\n",
    "                continue\n",
    "\n",
    "            # Pattern could be legal form suffix like \"AG\", \"GmbH\" etc.\n",
    "            if tl in self.org_suffixes:\n",
    "                #go backwards to find start of name chunk\n",
    "                start = i\n",
    "                while start - 1 >= 0 and tokens[start - 1][0].isupper() and labels[start - 1] == \"O\":\n",
    "                    start -= 1\n",
    "                labels[start] = \"B-ORG\"\n",
    "                for j in range(start + 1, i + 1):\n",
    "                    if labels[j] == \"O\":\n",
    "                        labels[j] = \"I-ORG\"\n",
    "\n",
    "        return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b17e62f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_model = SimpleRuleNER()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6ccab8",
   "metadata": {},
   "source": [
    "# Token-Level Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27db7a5",
   "metadata": {},
   "source": [
    "here we will build a Naive-Bayes but on the token-level - this version accounts for class imbalance right now\n",
    "here we should probably do one version where we do not account for class imbalance -> almost always predict O and have good F1 -> error analysis -> account for class imbalance -> better predictions, but lower F1 score! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad13e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenNB:\n",
    "    def __init__(self):\n",
    "        self.labels = set()\n",
    "        self.word_count = Counter()              # global word freq\n",
    "        self.count_by_label = defaultdict(Counter)  # label -> word -> count\n",
    "        self.label_count = Counter()             # label -> token count\n",
    "        self.trained = False\n",
    "        self.weights = {}\n",
    "        self.label_priors = {}\n",
    "        self.majority_label = \"O\"\n",
    "\n",
    "    def count_tokens(self, token_label_pairs):\n",
    "        \"\"\"\n",
    "        token_label_pairs: iterable of (word, label) pairs\n",
    "        \"\"\"\n",
    "        for word, label in token_label_pairs:\n",
    "            self.labels.add(label)\n",
    "            self.word_count[word] += 1\n",
    "            self.count_by_label[label][word] += 1\n",
    "            self.label_count[label] += 1\n",
    "\n",
    "    def calculate_weights(self):\n",
    "        \"\"\"\n",
    "        Compute log P(word | label) and (optionally) log priors.\n",
    "        \"\"\"\n",
    "        V = len(self.word_count)\n",
    "\n",
    "        # log P(word | label) with Laplace smoothing\n",
    "        self.weights = {}\n",
    "        for word in self.word_count:\n",
    "            self.weights[word] = {}\n",
    "            for label in self.labels:\n",
    "                num = self.count_by_label[label][word] + 1\n",
    "                denom = self.label_count[label] + V\n",
    "                self.weights[word][label] = log(num / denom)\n",
    "\n",
    "        # ---- PRIOR HANDLING ----\n",
    "        # Option 1: uniform priors  -> neutral w.r.t. imbalance\n",
    "        self.label_priors = {label: 0.0 for label in self.labels}\n",
    "\n",
    "        if self.label_count:\n",
    "            self.majority_label = self.label_count.most_common(1)[0][0]\n",
    "        else:\n",
    "            self.majority_label = \"O\"\n",
    "\n",
    "        self.trained = True\n",
    "\n",
    "    def predict_token(self, word):\n",
    "        \"\"\"\n",
    "        Predict a label for a single token.\n",
    "        \"\"\"\n",
    "        if not self.trained:\n",
    "            raise RuntimeError(\"Call calculate_weights() first\")\n",
    "\n",
    "        # unseen word -> fall back to majority label (probably 'O')\n",
    "        if word not in self.weights:\n",
    "            return self.majority_label\n",
    "\n",
    "        best_label = None\n",
    "        best_score = float(\"-inf\")\n",
    "        for label in self.labels:\n",
    "            score = self.label_priors[label] + self.weights[word][label]\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_label = label\n",
    "        return best_label\n",
    "\n",
    "    def predict_sentence(self, tokens):\n",
    "        \"\"\"\n",
    "        tokens: list of word strings\n",
    "        returns: list of predicted labels\n",
    "        \"\"\"\n",
    "        return [self.predict_token(w) for w in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b914aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, sentences, name=\"\"):\n",
    "    \"\"\"\n",
    "    sentences: list of dicts with keys \"tokens\" and \"labels\"\n",
    "               - \"tokens\": list[str]\n",
    "               - \"labels\": list[str] (gold BIO tags)\n",
    "    \"\"\"\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for sent in sentences:\n",
    "        gold = sent[\"labels\"]\n",
    "        pred = model.predict_sentence(sent[\"tokens\"])\n",
    "        if len(pred) != len(gold):\n",
    "            raise ValueError(\"Length mismatch between gold and prediction\")\n",
    "        y_true.extend(gold)\n",
    "        y_pred.extend(pred)\n",
    "\n",
    "    print(f\"=== {name} ===\")\n",
    "    print(classification_report(y_true, y_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf71f351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SimpleRuleNER ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LEG      1.000     0.500     0.667        12\n",
      "       B-MON      0.000     0.000     0.000         2\n",
      "       B-ORG      0.000     0.000     0.000         5\n",
      "       I-LEG      0.000     0.000     0.000        23\n",
      "       I-MON      0.000     0.000     0.000         1\n",
      "       I-ORG      0.000     0.000     0.000        10\n",
      "           O      0.931     1.000     0.964       638\n",
      "\n",
      "    accuracy                          0.932       691\n",
      "   macro avg      0.276     0.214     0.233       691\n",
      "weighted avg      0.877     0.932     0.902       691\n",
      "\n",
      "=== TokenNB (uniform prior) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LEG      0.700     0.583     0.636        12\n",
      "       B-MON      0.000     0.000     0.000         2\n",
      "       B-ORG      1.000     0.400     0.571         5\n",
      "       I-LEG      0.435     0.435     0.435        23\n",
      "       I-MON      0.011     1.000     0.021         1\n",
      "       I-ORG      0.545     0.600     0.571        10\n",
      "           O      0.962     0.829     0.891       638\n",
      "\n",
      "    accuracy                          0.803       691\n",
      "   macro avg      0.522     0.550     0.446       691\n",
      "weighted avg      0.930     0.803     0.860       691\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\timgr\\Desktop\\NLP\\NLP-Group-23\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\timgr\\Desktop\\NLP\\NLP-Group-23\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\timgr\\Desktop\\NLP\\NLP-Group-23\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\timgr\\Desktop\\NLP\\NLP-Group-23\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\timgr\\Desktop\\NLP\\NLP-Group-23\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\timgr\\Desktop\\NLP\\NLP-Group-23\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "rain_tokens = [\n",
    "    (w, l)\n",
    "    for sent in train_set\n",
    "    for w, l in zip(sent[\"tokens\"], sent[\"labels\"])\n",
    "]\n",
    "\n",
    "nb_model = TokenNB()\n",
    "nb_model.count_tokens(train_tokens)\n",
    "nb_model.calculate_weights()\n",
    "\n",
    "evaluate_model(rule_model, dev_set, name=\"SimpleRuleNER\")\n",
    "evaluate_model(nb_model, dev_set, name=\"TokenNB (uniform prior)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
